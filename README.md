# ğŸ’¬ AI Comment Toxicity Detection System

### ğŸ“Œ Overview

This project builds a Deep Learning-based system to detect toxic comments in online platforms. The model classifies comments as Toxic or Non-Toxic and is deployed using Streamlit for real-time and bulk predictions.


## ğŸ§  Key Features

âœ” Real-Time Toxicity Detection  
âœ” Bulk CSV Moderation  
âœ” Deep Learning Model Comparison (LSTM vs CNN)  
âœ” Dataset Insights & Visualization  
âœ” Glassmorphism UI with Dark/Light Mode  
âœ” Streamlit Cloud Deployment Ready  

## ğŸ›  Tech Stack

| Category | Tools Used |
|-----------|------------|
| Programming | Python |
| NLP | Tokenization, Stopword Removal, Padding |
| Deep Learning | LSTM, CNN |
| Framework | TensorFlow / Keras |
| Deployment | Streamlit |
| Visualization | Matplotlib |
| Version Control | GitHub |


## ğŸ“‚ Dataset

Dataset: **Jigsaw Toxic Comment Classification Dataset**

Original Labels:
- toxic  
- severe_toxic  
- obscene  
- threat  
- insult  
- identity_hate  


### Binary Target Creation

A unified binary label was created:


LSTM (Final Model) â€“ 96.13% Accuracy

CNN â€“ 95.71% Accuracy

LSTM performed better in contextual understanding and was selected for deployment.


Toxic (1) â†’ If any toxicity category = 1

Non-Toxic (0) â†’ Otherwise


## ğŸ” Project Workflow

### 1ï¸âƒ£ Data Exploration
- Dataset shape & structure analysis
- Missing value check
- Class imbalance analysis
- Sample toxic vs non-toxic inspection

### 2ï¸âƒ£ Text Preprocessing
- Lowercasing
- Special character removal
- Stopword removal
- Tokenization
- Sequence padding

### 3ï¸âƒ£ Model Development

Two architectures were implemented and compared:

#### ğŸ”¹ LSTM Model
- Embedding Layer
- LSTM Layer
- Dropout
- Dense (Sigmoid Output)

#### ğŸ”¹ CNN Model
- Embedding Layer
- Conv1D
- Global Max Pooling
- Dense (Sigmoid Output)

## ğŸ“Š Model Performance

| Model | Accuracy |
|--------|----------|
| LSTM | **96.13%** |
| CNN | 95.71% |

ğŸ“Œ LSTM achieved better contextual understanding and was selected for deployment.


## ğŸ’¾ Model Saving

Final Model: 
 
      final_toxicity_lstm_model.keras

Tokenizer:

      final_tokenizer.pkl


     
### â–¶ï¸ How to Run the Project

1ï¸âƒ£ Install dependencies:

      pip install -r requirements.txt

2ï¸âƒ£ Run Streamlit App:

      streamlit run app.py


 ### ğŸ“ Project Structure

          AI-Toxicity-Detection/
             â”‚
             â”œâ”€â”€ app.py
             â”œâ”€â”€ final_toxicity_lstm_model.keras
             â”œâ”€â”€ final_tokenizer.pkl
             â”œâ”€â”€ test.csv
             â”œâ”€â”€ train.csv
             â”œâ”€â”€ requirements.txt
             â””â”€â”€ README.md
