# ðŸ’¬ AI Comment Toxicity Detection System

### ðŸ“Œ Overview

This project builds a Deep Learning-based system to detect toxic comments in online platforms. The model classifies comments as Toxic or Non-Toxic and is deployed using Streamlit for real-time and bulk predictions.


## ðŸ§  Key Features

âœ” Real-Time Toxicity Detection  
âœ” Bulk CSV Moderation  
âœ” Deep Learning Model Comparison (LSTM vs CNN)  
âœ” Dataset Insights & Visualization  
âœ” Glassmorphism UI with Dark/Light Mode  
âœ” Streamlit Cloud Deployment Ready  

---

## ðŸ›  Tech Stack

| Category | Tools Used |
|-----------|------------|
| Programming | Python |
| NLP | Tokenization, Stopword Removal, Padding |
| Deep Learning | LSTM, CNN |
| Framework | TensorFlow / Keras |
| Deployment | Streamlit |
| Visualization | Matplotlib |
| Version Control | GitHub |

---

## ðŸ“‚ Dataset

Dataset: **Jigsaw Toxic Comment Classification Dataset**

Original Labels:
- toxic  
- severe_toxic  
- obscene  
- threat  
- insult  
- identity_hate  

### Binary Target Creation

A unified binary label was created:


LSTM (Final Model) â€“ 96.13% Accuracy

CNN â€“ 95.71% Accuracy

LSTM performed better in contextual understanding and was selected for deployment.


